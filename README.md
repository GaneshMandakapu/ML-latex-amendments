# `Machine Learning` ![](https://img.shields.io/badge/Made%20with-LaTeX-1f425f.svg)
Contains coursework assignments in Masters made in latex.
Includes solved numericals, understanding questions and some extra topics.

- `A1.1` - Machine Learning and its applications
- `A1.2` - Least Mean Square (LMS) algorithm 
- `A1.3` - Confusion matrix and metrics
- `A1.4` - Learning system for a _Tic-Tac-Toe_ player
- `A2.1` - Match the followiing algorithms and loss functions to their classification counterparts
- `A2.2` - The `Bias-Variance` tradeoff
- `A2.3` - Categorical and Numerical features in a dataset
- `A2.4` - Maximum Likelihood Estimates (MLE) for the _Univariate Gaussian Distribution_
- `A3.1` - Concept learning and related disciplines
- `A3.2` - Use case of concept learning _Addison's disease_
- `A3.3` - `Find-S` algorithm and `Candidate-Elimination` algorithm
- `A3.4` - Cross validation as an classifier evaluation technique
- `A4.1` - Concept learning for `Decision Trees`
- `A4.2` - Decision Tree basics for Machine Learning
- `A4.3` - Feature selection and challenges for Decision trees ( _use case_ )
- `A4.4` - Iterative Dichotomiser-3 `ID-3` algorithm
- `A5.1` - Overfitting in Decision Trees with relation to `Bias` & `Variance`
- `A5.2` - Tree pruning for decision trees ( _Reduced Error Pruning_ )
- `A5.3` - Gain ratio as split measure 
- `A5.4` - Regression Trees 
- `A6.1` - Perceptron for classification
- `A6.2` - The Perceptron training rule ( _Delta rule_ )
- `A6.3` - Neural Networks and its modalities
- `A6.4` - Activation functions for Neural Networks ( _ReLU, Leaky ReLU variants_ )
- `A7.1` - Gradient descent training rule
- `A7.2` - Proper `loss` functions for `activation` functions
- `A7.3` - The Backpropogation algorithm [Video](https://www.youtube.com/watch?v=jIdXHu78XGY&list=PLPN-43XehstM4-SWLIUS5eFxPmFJ3iHan&index=9&ab_channel=RANJIRAJ)
- `A7.4` - Effect of Learning rate as hyperparameter
- `A8.1` - Non-sequential data classifiers, Feed-forward Neural Networks, BPTT, LSTM
- `A8.2` - Naive bayes and Maximum-Aposteriori-Hypothesis (MAP)
- `A8.3` - Naive Bayes ( _Numerical_ )
- `A8.4` - Spam classification `SpamAssassin`
- `A9.1` - The `k-Nearest Neighbor` algorithm
- `A9.2` - Regression & Classification algorithms
- `A9.3` - _k_-NN ( _Numerical_ )
- `A9.4` - Active Learning for Case-based reasoning
- `A10.1` - Supervised vs. Unsupervised learning 
- `A10.2` - _k_ Means algorithm in action
- `A10.3` - Hierachical Agglomerative Clustering algorithm
- `A10.4` - Fuzzy-C-Means algorithm
- `A11.1` - `Learning Vector Quantization (LVQ)` algorithm
- `A11.2` - Reinforcement Learning and its components
- `A11.3` - The `Value-Iteration` algorithm
- `A11.4` - The `Value-Iteration` algorithm ( _Episodic process_ )
- `A12.1` - Association rules
- `A12.2` - Frequent Itemset Mining ( _Exercise_ )
- `A12.3` - Support, Confidence measures for Arules ( _Numerical_ )
- `A12.4` - Apriori vs. ECLAT
